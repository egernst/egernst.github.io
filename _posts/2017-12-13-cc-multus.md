---
layout: post
title: First look at Clear Containers with Multus
category: posts
---


Finally got around to testing multus with Clear Containers. One thing that hung me up for
a few minutes was that I needed to create a default Multus configuration in /etc/cni/net.d
even though I was creating and making use of CRD definitions. I ended up using a simple flannel default as seen [here](../raw-files/multus-cc-ex/01-multus-cni.conf) 

To start up the cluster, we run:
```
sudo cp 01-multus-cni.conf /etc/cni/net.d/
sudo -E kubeadm init --pod-network-cidr 10.244.0.0/16
export KUBECONFIG=/etc/kubernetes/admin.conf
```

Next, we create the CRD network definition using [crdnetwork.yaml](../raw-files/multus-cc-ex/crdnetwork.yaml).

We then create networks making use of this CRD, using [example-networks.yaml](../raw-files/multus-cc-ex/example-networks.yaml).

Next, create two pods: a [trusted](../raw-files/multus-cc-ex/example-pod.yaml) and [untrusted](../raw-files/multus-cc-ex/example-pod-untrusted.yaml) pod, each with a single container, a 14.04 Ubuntu
image.  These pods make use of the CRD annotations to request 3 bridge networks and one PTP
network.  Assuming you setup your kubernetes to use clear containers as desribed in [this Medium article](https://medium.com/cri-o/intel-clear-containers-and-cri-o-70824fb51811) and [this wiki](https://github.com/clearcontainers/runtime/wiki/Clear-Containers-and-Kubernetes),
the untrusted pod will use Clear Containers.

Doing an exec shows all of the expected interfaces:

```
$ sudo -E kubectl exec -it crd-network-test-cc -- ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 0a:58:0a:01:0a:03 brd ff:ff:ff:ff:ff:ff
    inet 10.1.10.3/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::858:aff:fe01:a03/64 scope link 
       valid_lft forever preferred_lft forever
3: net0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 0a:58:0b:01:01:04 brd ff:ff:ff:ff:ff:ff
    inet 11.1.1.4/24 scope global net0
       valid_lft forever preferred_lft forever
    inet6 fe80::858:bff:fe01:104/64 scope link 
       valid_lft forever preferred_lft forever
4: net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 0a:58:0b:01:01:05 brd ff:ff:ff:ff:ff:ff
    inet 11.1.1.5/24 scope global net1
       valid_lft forever preferred_lft forever
    inet6 fe80::858:bff:fe01:105/64 scope link 
       valid_lft forever preferred_lft forever
5: net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 0a:58:0a:f8:f6:93 brd ff:ff:ff:ff:ff:ff
    inet 10.248.246.147/28 scope global net2
       valid_lft forever preferred_lft forever
    inet6 fe80::858:aff:fef8:f693/64 scope link 
       valid_lft forever preferred_lft forever
6: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1
    link/sit 0.0.0.0 brd 0.0.0.0
```


Checking out the running QEMU process on our system shows the four network interfaces included:
```
root      30019  29990 17 14:48 ?        00:00:04 /usr/bin/qemu-lite-system-x86_64 
 -name pod-d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b
 -uuid 359846b2-92c7-4918-adc3-656a3c0c8699
 -machine pc,accel=kvm,kernel_irqchip,nvdimm
 -cpu host
 -qmp unix:/run/virtcontainers/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b/359846b2-92c7-491,server,nowait
 -qmp unix:/run/virtcontainers/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b/359846b2-92c7-491,server,nowait
 -m 2048M,slots=2,maxmem=97635M
 -smp 32,cores=32,threads=1,sockets=1
 -device virtio-9p-pci,fsdev=ctr-9p-0,mount_tag=ctr-rootfs-0
 -fsdev local,id=ctr-9p-0,path=/var/lib/containers/storage/overlay/0cfd05f6880cef4f163faded98b898320eef1fcfc439d20b2f53746afeee7559/merged,security_model=none
 -device virtio-serial-pci,id=serial0
 -device virtconsole,chardev=charconsole0,id=console0
 -chardev socket,id=charconsole0,path=/run/virtcontainers/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b/console.sock,server,nowait
 -device nvdimm,id=nv0,memdev=mem0
 -object memory-backend-file,id=mem0,mem-path=/usr/share/clear-containers/clear-19350-containers.img,size=235929600
 -device pci-bridge,bus=pci.0,id=pci-bridge-0,chassis_nr=1,shpc=on
 -device virtserialport,chardev=charch0,id=channel0,name=sh.hyper.channel.0
 -chardev socket,id=charch0,path=/run/virtcontainers/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b/hyper.sock,server,nowait
 -device virtserialport,chardev=charch1,id=channel1,name=sh.hyper.channel.1
 -chardev socket,id=charch1,path=/run/virtcontainers/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b/tty.sock,server,nowait
 -device virtio-9p-pci,fsdev=extra-9p-hyperShared,mount_tag=hyperShared
 -fsdev local,id=extra-9p-hyperShared,path=/tmp/hyper/shared/pods/d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b,security_model=none
 -netdev tap,id=network-0,vhost=on,fds=3:4:5:6:7:8:9:10
 -device driver=virtio-net-pci,netdev=network-0,mac=0a:58:0a:01:0a:03,mq=on,vectors=18
 -netdev tap,id=network-1,vhost=on,fds=11:12:13:14:15:16:17:18
 -device driver=virtio-net-pci,netdev=network-1,mac=0a:58:0b:01:01:04,mq=on,vectors=18
 -netdev tap,id=network-2,vhost=on,fds=19:20:21:22:23:24:25:26
 -device driver=virtio-net-pci,netdev=network-2,mac=0a:58:0b:01:01:05,mq=on,vectors=18
 -netdev tap,id=network-3,vhost=on,fds=27:28:29:30:31:32:33:34
 -device driver=virtio-net-pci,netdev=network-3,mac=0a:58:0a:f8:f6:93,mq=on,vectors=18
 -rtc base=utc,driftfix=slew
 -global kvm-pit.lost_tick_policy=discard
 -vga none
 -no-user-config
 -nodefaults
 -nographic
 -daemonize
 -kernel /usr/share/clear-containers/vmlinuz-4.9.60-80.container
 -append root=/dev/pmem0p1 rootflags=dax,data=ordered,errors=remount-ro rw rootfstype=ext4 tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k panic=1 console=hvc0 console=hvc1 initcall_debug iommu=off cryptomgr.notests net.ifnames=0 debug systemd.show_status=true systemd.log_level=debug init=/usr/lib/systemd/systemd systemd.unit=clear-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket ip=::::::d17bffa85322eae0679d0cc3586df81ac507718ff1f1824a33aa50f325fc5a5b::off::
```


## Quick and easy reproduction:
Assuming you already have a kubernetes+crio+ clear container setup on your host system, you
can quickly reproduce as follows:

```
wget https://egernst.github.io/raw-files/multus-cc-ex/01-multus-cni.conf
sudo cp 01-multus-cni.conf /etc/cni/net.d/
sudo -E kubeadm init --pod-network-cidr 10.244.0.0/16
export KUBECONFIG=/etc/kubernetes/admin.conf
# wait for pods to come up; once up press ctl-c:
sudo -E kubectl get pods --all-namespaces -w -o wide

wget https://egernst.github.io/raw-files/multus-cc-ex/crdnetwork.yaml
sudo -E kubectl create -f crdnetwork.yaml
wget https://egernst.github.io/raw-files/multus-cc-ex/example-networks.yaml
sudo -E kubectl create -f example-networks.yaml

# taint this master node so we can run jobs on it:
master=$(hostname)
sudo -E kubectl taint nodes "$master" node-role.kubernetes.io/master:NoSchedule-

wget https://egernst.github.io/raw-files/multus-cc-ex/example-pod.yaml
wget https://egernst.github.io/raw-files/multus-cc-ex/example-pod-untrusted.yaml
sudo -E kubectl create -f example-pod.yaml
sudo -E kubectl create -f example-pod-untrusted.yaml
sudo -E kubectl exec -it crd-network-test -- ip a
sudo -E kubectl exec -it crd-network-test-cc -- ip a
```

